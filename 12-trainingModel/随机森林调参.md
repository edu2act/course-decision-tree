# 随机森林调参

我们学习了随机森林(Random Forest, 以下简称RF）的原理与编程实现。下面对RF的使用做总结。重点讲述scikit-learn中RF的调参注意事项。

## 1. scikit-learn随机森林类库概述

在sklearn中，RF的分类器是`RandomForestClassifier`，回归类是`RandomForestRegressor`。当然RF的变种Extra Trees也有， 分类类ExtraTreesClassifier，回归类ExtraTreesRegressor。由于RF和Extra Trees的区别较小，调参方法基本相同，本文只关注于RF的调参。

RF需要调参的参数包括两大部分，第一部分是Bagging框架的参数，第二部分是CART决策树的参数。下面我们就对这些参数做一个介绍。

## 2. RF框架参数

​	下面我来看RF重要的Bagging框架的参数，由于`RandomForestClassifier`和`RandomForestRegressor`参数绝大部分相同，这里会将它们一起讲，不同点会指出。

​	1) n_estimators: 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是100。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。

​	2) oob_score :即是否采用袋外样本来评估模型的好坏。默认识False。一般设置为True，因为袋外分数反应了一个模型拟合后的泛化能力。

​	3) criterion: 即CART树做划分时对特征的评价标准。分类模型和回归模型的损失函数是不一样的。分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益entropy。回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae。一般来说选择默认的标准就已经很好的。

​	从上面可以看出， RF重要的框架参数比较少，主要需要关注的是 **n_estimators**，即RF最大的决策树个数。

## 3. RF决策树参数

​	1) RF划分时考虑的最大特征数max_features: 可以使用很多种类型的值，默认是"None",意味着划分时考虑所有的特征数；如果是"log2"意味着划分时最多考虑log2(N)个特征；如果是"sqrt"或者"auto"意味着划分时最多考虑sqrt(N)个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。一般来说，如果样本特征数不多，比如小于50，我们用默认的"None"就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。

​	2) 决策树最大深度max_depth: 默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，j能够减少运行时间，也可以防止过拟合现象。常用的可以取值10-100之间。

​	3) 内部节点再划分所需最小样本数min_samples_split	: 这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2，如果样本量不大，不需要管这个值。如果样本量数量级非常大，则需要增大这个值。

​	4) 叶子节点最少样本数min_samples_leaf: 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。

​	5）叶子节点最小的样本权重和min_weight_fraction_leaf：这个值限制了叶子节点中样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。 默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。

​	6) 最大叶子节点数max_leaf_nodes: 通过限制最大叶子节点数，可以防止过拟合，默认是"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。

​	7) 节点划分最小不纯度min_impurity_split:  这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。一般不推荐改动默认值1e-7。

​	上面决策树参数中最重要的包括最大特征数max_features， 最大深度max_depth， 内部节点再划分所需最小样本数min_samples_split和叶子节点最少样本数min_samples_leaf。

## 4.调参实例

使用随机森林对水雷-岩石数据、红酒口感数据进行分类，使用的评价标准是袋外分数和AUC分数

## 

```python
#模型
from sklearn.ensemble import RandomForestClassifier
#评价方法AUC metrics.roc_auc_score
from sklearn import metrics
#调参方法
from sklearn.model_selection import cross_val_score
#数据打乱
from sklearn.utils import shuffle

#首先查看分类的分布情况
#查看特征属性
#查看连续特征的分布情况：箱线图
    #删除异常点
#查看特征之间的相关性-包括各个特征与标签的相关性
    #特征选择：特征删除，特征组合
#缺失值的填充
#模型调参
```







